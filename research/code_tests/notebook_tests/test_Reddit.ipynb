{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4924761d",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Test the Reddit data collection class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ca42a",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4cf6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T16:35:07.734895Z",
     "iopub.status.busy": "2024-03-04T16:35:07.730436Z",
     "iopub.status.idle": "2024-03-04T16:35:07.779212Z",
     "shell.execute_reply": "2024-03-04T16:35:07.777739Z",
     "shell.execute_reply.started": "2024-03-04T16:35:07.734302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "# Environmental variables\n",
    "import dotenv\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Reddit class\n",
    "reddit_class_path = '../../../code/reddit'\n",
    "sys.path.insert(0, reddit_class_path)\n",
    "import reddit_data_fetcher as rdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e821cdd9-ff0c-4826-9b80-199876a93816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T16:35:08.159849Z",
     "iopub.status.busy": "2024-03-04T16:35:08.158797Z",
     "iopub.status.idle": "2024-03-04T16:35:08.169031Z",
     "shell.execute_reply": "2024-03-04T16:35:08.167947Z",
     "shell.execute_reply.started": "2024-03-04T16:35:08.159819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from collections import deque\n",
    "# import itertools\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Parameters\n",
    "# api_call_times = deque()\n",
    "# rate_limit_window = timedelta(minutes=1)\n",
    "# api_call_limit = 44\n",
    "# pause_indexes = deque()\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # print(i)\n",
    "    \n",
    "#     t = datetime(2024, 3, 1, 13, 58, 23) + timedelta(seconds=i)\n",
    "#     api_call_times.append(t)\n",
    "    \n",
    "#     # We need to pause at least some time\n",
    "#     if len(api_call_times) == api_call_limit * (len(pause_indexes) + 1):\n",
    "        \n",
    "#         # Add this pause to the indexes of pauses\n",
    "#         pause_indexes.append(len(api_call_times) - 1) \n",
    "        \n",
    "#         # If this is the first pause wait until time uses first API entry\n",
    "#         if len(pause_indexes) == 0:\n",
    "\n",
    "            \n",
    "#             wait_until = api_call_times[0] + rate_limit_window\n",
    "\n",
    "#         # otherwise use the time of the last pause\n",
    "#         else:\n",
    "#             wait_until = api_call_times[pause_indexes[-1]] + rate_limit_window\n",
    "            \n",
    "\n",
    "        \n",
    "#         print('waiting:       t: {}, len(api_call_times): {}, pause_indexes[-1]: {}'.format(t, \n",
    "#                                                                                       len(api_call_times),\n",
    "#                                                                                       pause_indexes[-1]))\n",
    "#         print('pause time check: {}'.format(api_call_times[pause_indexes[-1]]))\n",
    "#         print('wait until check: {}'.format(api_call_times[pause_indexes[-1]]+rate_limit_window ))        \n",
    "#         print('      wait until: {}'.format(wait_until))\n",
    "#         print()\n",
    "\n",
    "\n",
    "# for t in deque(itertools.islice(api_call_times,0,8)):\n",
    "#     print(t.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31c114",
   "metadata": {},
   "source": [
    "## Check the logic for cleaning keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfe42de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T16:35:09.473267Z",
     "iopub.status.busy": "2024-03-04T16:35:09.472416Z",
     "iopub.status.idle": "2024-03-04T16:35:09.493831Z",
     "shell.execute_reply": "2024-03-04T16:35:09.493101Z",
     "shell.execute_reply.started": "2024-03-04T16:35:09.473210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_keyword_text(text):\n",
    "    '''\n",
    "    Method to clean Reddit and keyword texts in either the original post\n",
    "    or associated comments.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Remove these words from search terms\n",
    "    stop_words = ['stop', 'the', 'to', 'and', 'a', 'in', 'it',\n",
    "                  'is', 'I', 'i', 'that', 'had', 'on', 'for', 'were', 'was',\n",
    "                  'through', 'of', 'way', 'end', 'our', 'place', 'home',\n",
    "                  'support', 'city', 'visitor', 'women', 'men', 'need', 'idea',\n",
    "                  'north', 'south', 'east', 'west', 'ready', 'save', 'salt', 'win',\n",
    "                  'lose', 'loss', 'family', 'working', 'hope', 'love', 'house']\n",
    "\n",
    "#     # Split words with some characters\n",
    "# #     pat = r\"\\/\"\n",
    "#     pat = \"\\\\/\"\n",
    "#     text = re.sub(pat, ' ', text)\n",
    "\n",
    "    # Remove unwanted characters\n",
    "\n",
    "    pat = r\"\\\\n|r/|[^a-zA-Z0-9 ]\"\n",
    "    text = re.sub(pat, ' ', text)\n",
    "\n",
    "    # cohvert to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # Remove stop words\n",
    "    pat = \"|\".join([\"\\\\b{}\\\\b\".format(w) for w in stop_words])\n",
    "    text = re.sub(pat, ' ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    pat = r\"\\s{2,}\"\n",
    "    text = re.sub(pat, ' ', text)\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab0dab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T16:51:13.268327Z",
     "iopub.status.busy": "2024-03-04T16:51:13.266695Z",
     "iopub.status.idle": "2024-03-04T16:51:13.288740Z",
     "shell.execute_reply": "2024-03-04T16:51:13.288086Z",
     "shell.execute_reply.started": "2024-03-04T16:51:13.268259Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords_file_path = '../../../data/keywords'\n",
    "keywords_files = ['ac.csv']\n",
    "keywords_files = ['keywords.csv', 'hashtags_other.csv']\n",
    "\n",
    "keywords = []\n",
    "\n",
    "# read each file with keywords\n",
    "for kwf in keywords_files:\n",
    "\n",
    "    # Read the files into a dataframe\n",
    "    df_kw = pd.read_csv(os.path.join(keywords_file_path, kwf), index_col=0)\n",
    "\n",
    "    # Get the cleaned keywords\n",
    "    keywords.extend([clean_keyword_text(k) for k in df_kw[df_kw.columns[0]].tolist()])\n",
    "\n",
    "# Ensure keywords are strings and remove any duplicates\n",
    "search_terms = [k for k in set(keywords) if isinstance(k, str) and len(k) > 1]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b77e173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T16:51:14.262054Z",
     "iopub.status.busy": "2024-03-04T16:51:14.261237Z",
     "iopub.status.idle": "2024-03-04T16:51:14.271605Z",
     "shell.execute_reply": "2024-03-04T16:51:14.271027Z",
     "shell.execute_reply.started": "2024-03-04T16:51:14.262002Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poverty',\n",
       " 'vicpol',\n",
       " 'addict addicted',\n",
       " 'bcpoli',\n",
       " 'yyjpoli',\n",
       " 'poor',\n",
       " 'shelter',\n",
       " 'camp',\n",
       " 'alcoholic',\n",
       " 'social structure',\n",
       " 'overdose',\n",
       " 'collude',\n",
       " 'thief',\n",
       " 'victoria bc',\n",
       " 'social housing',\n",
       " 'yyj',\n",
       " 'victoriabc',\n",
       " 'homelessness',\n",
       " 'theft',\n",
       " 'housing',\n",
       " 'low income',\n",
       " 'social problem',\n",
       " 'violence',\n",
       " 'victoriabuzz',\n",
       " 'crime',\n",
       " 'unhoused',\n",
       " 'britishcolumbia',\n",
       " 'narcotics',\n",
       " 'housing insecure',\n",
       " 'homeless',\n",
       " 'camping',\n",
       " 'affordable housing',\n",
       " 'substance use',\n",
       " 'encampment',\n",
       " 'camper',\n",
       " 'drugs',\n",
       " 'evict',\n",
       " 'stolen',\n",
       " 'affordable',\n",
       " 'alliance homelessness capital region',\n",
       " 'tent']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35796a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T01:38:43.137403Z",
     "iopub.status.busy": "2024-03-04T01:38:43.137330Z",
     "iopub.status.idle": "2024-03-04T01:38:43.139931Z",
     "shell.execute_reply": "2024-03-04T01:38:43.139670Z",
     "shell.execute_reply.started": "2024-03-04T01:38:43.137396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addicted addict pppp newline'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"addicted/addict\"\n",
    "clean_keyword_text(test_text)\n",
    "\n",
    "test_text = \"addicted/addict$#pppp-newline/r/\"\n",
    "clean_keyword_text(test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57f4dd",
   "metadata": {},
   "source": [
    "## Test the Reddit crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5467d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T01:38:43.140405Z",
     "iopub.status.busy": "2024-03-04T01:38:43.140336Z",
     "iopub.status.idle": "2024-03-04T01:38:43.142971Z",
     "shell.execute_reply": "2024-03-04T01:38:43.142730Z",
     "shell.execute_reply.started": "2024-03-04T01:38:43.140398Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv('../../../data/environment/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a630ae0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-04T01:38:43.143609Z",
     "iopub.status.busy": "2024-03-04T01:38:43.143525Z",
     "iopub.status.idle": "2024-03-04T01:39:01.003150Z",
     "shell.execute_reply": "2024-03-04T01:39:01.002068Z",
     "shell.execute_reply.started": "2024-03-04T01:38:43.143602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2024-03-03 17:38:43: event: start fetch: subreddit_name: britishcolumbia\n",
      "time: 2024-03-03 17:39:00: event: fetch complete\n"
     ]
    }
   ],
   "source": [
    "# Attributes to modify using the **kwargs parameter\n",
    "posts_file_path = \"../../data_tests/reddit_tests/posts\"\n",
    "logs_file_path = \"../../data_tests/reddit_tests/logs\"\n",
    "keywords_file_path = '../../../data/keywords'\n",
    "limit_num=10\n",
    "\n",
    "\n",
    "# Initialize Reddit object\n",
    "data_fetcher = rdf.GVCEHReddit(client_id=os.environ.get(\"REDDIT_CLIENT_ID\"),\n",
    "                              client_secret=os.environ.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "                              user_agent=os.environ.get(\"REDDIT_USER_AGENT\"),\n",
    "                              posts_file_path=posts_file_path,\n",
    "                              logs_file_path=logs_file_path,\n",
    "                              keywords_file_path=keywords_file_path,\n",
    "                              limit_num=limit_num)\n",
    "\n",
    "#Define subreddits to search\n",
    "subreddit_names = ['britishcolumbia']\n",
    "\n",
    "# await data_fetcher.fetch_search_data(subreddit_names=subreddit_names)\n",
    "await data_fetcher.fetch_new_data(subreddit_names=subreddit_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2fb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d030f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb46e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
